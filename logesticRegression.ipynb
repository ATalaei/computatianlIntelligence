{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27b3208",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clean1.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         sigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (X \u001b[38;5;241m-\u001b[39m mu) \u001b[38;5;241m/\u001b[39m sigma\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean1.data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     24\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean1.data'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalizer\n",
    "\n",
    "def normalize(X):\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        return (X - mu) / sigma\n",
    "with open('clean1.data', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    data_list = [row for row in reader]\n",
    "\n",
    "data_array = np.array(data_list)\n",
    "\n",
    "X_test=data_array[ : , 2:]\n",
    "X_test=X_test[ :, :-1]\n",
    "y_test=data_array[ : , 0:1]\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i][0][0] == 'M'):\n",
    "        y_test[i]=1\n",
    "    elif(y_test[i][0][0]==\"N\"):\n",
    "        y_test[i]=0\n",
    "\n",
    "X_test = X_test.astype(np.float64)\n",
    "y_test = y_test.astype(np.float64)\n",
    "X_test=normalize(X_test)\n",
    "class Stochastic_Logestic_Reg(object):\n",
    "\n",
    "    def __init__(self, learning_rate=.01, n_epochs=1000, cutoff=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.cutoff = cutoff\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "    #\n",
    "    # def __repr__(self):\n",
    "    #     params = {\n",
    "    #         'learning_rate': self.learning_rate,\n",
    "    #         'n_epochs': self.n_epochs,\n",
    "    #         'cutoff': self.cutoff\n",
    "    #     }\n",
    "    #     return \"SLR({0}={3}, {1}={4}, {2}={5})\".format(*params.keys(), *params.values())\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, row):\n",
    "        z = np.dot(row, self.w) + self.b\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        self.predict_probas = []\n",
    "        for i in range(X.shape[0]):\n",
    "            ypred = self.predict_proba(X[i])\n",
    "            self.predict_probas.append(ypred)\n",
    "\n",
    "        return (np.array(self.predict_probas) >= self.cutoff) * 1.0\n",
    "\n",
    "    def score(self, X, y):\n",
    "        ypred = self.predict(X)\n",
    "\n",
    "        # y = y.to_numpy()\n",
    "\n",
    "        return accuracy_score(y, ypred)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.cost = []\n",
    "\n",
    "        self.m = X.shape[0]\n",
    "        self.log_loss = {}\n",
    "        self.cost = []\n",
    "        s = []\n",
    "        for n_epoch in range(1, self.n_epochs + 1):\n",
    "            losses = []\n",
    "\n",
    "            for i in range(self.m):\n",
    "                yhat = self.predict_proba(X[i])\n",
    "                grad_b = yhat - y[i]\n",
    "                grad_w =  X[i] * (yhat - y[i])\n",
    "\n",
    "                self.w -= self.learning_rate * grad_w / self.m\n",
    "                self.b -= self.learning_rate * grad_b / self.m\n",
    "                losses = -1/self.m * (y[i] * np.log(yhat) + (1 - y[i]) * np.log(1 - yhat))\n",
    "\n",
    "                #\n",
    "            s.append(self.score(X_test, y_test))\n",
    "            self.cost.append(sum(losses))\n",
    "\n",
    "        plt.plot(s)\n",
    "        plt.title('scores over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('scores')\n",
    "        plt.show()\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', Stochastic_Logestic_Reg(learning_rate=0.1, n_epochs=100, cutoff=0.5))\n",
    "])\n",
    "\n",
    "###################\n",
    "#reading darta#####\n",
    "###################\n",
    "import csv\n",
    "with open('clean2.data', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    data_list = [row for row in reader]\n",
    "\n",
    "data_array = np.array(data_list)\n",
    "\n",
    "X=data_array[ : , 2:]\n",
    "X=X[ :, :-1]\n",
    "y=data_array[ : , 0:1]\n",
    "\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if (y[i][0][0] == 'M'):\n",
    "        y[i]=1\n",
    "    elif(y[i][0][0]==\"N\"):\n",
    "        y[i]=0\n",
    "\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.float64)\n",
    "#####################\n",
    "#normalize###########\n",
    "#####################\n",
    "X=normalize(X)\n",
    "\n",
    "######################\n",
    "#learn model###########\n",
    "\n",
    "\n",
    "\n",
    "pipe1.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "accuracies1 = {\n",
    "    'train accuracy': pipe1.score(X,y),\n",
    "    'test accuracy': pipe1.score(X_test, y_test)\n",
    "}\n",
    "print(y_test.shape)\n",
    "# print(\"Accuracy score : \",accuracy_score(predict_probas, y_test))\n",
    "print('f1_score : ',f1_score(y_test, pipe1.predict(X_test), average='macro'))\n",
    "print(\"recall score : \",recall_score(y_test,pipe1.predict(X_test) , average='macro'))\n",
    "\n",
    "print(*accuracies1.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
